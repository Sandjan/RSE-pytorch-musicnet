{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import signal\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from musicnet_dataset import MusicNet\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "N_EPOCHS = 2000     \n",
    "EPOCH_SIZE = 2_000 \n",
    "EVAL_SIZE = 1_000\n",
    "BATCH_SIZE = 16\n",
    "SMOOTH = 0.01\n",
    "kwargs = {'pin_memory': True}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2048\n",
    "n_hidden= 48*4\n",
    "learning_rate = 0.00125 * np.sqrt(96 / n_hidden)\n",
    "lr_plateau_factor = 0.7\n",
    "lr_plateau_patience = 20\n",
    "epsilon = 1e-5 #like tf code\n",
    "weight_decay = 0.01 #like in tf code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#Not in use because it seems to never decrease the lr \n",
    "class CustomLRScheduler(ReduceLROnPlateau):  # like the scheduler in the tensorflow code\n",
    "    def __init__(self, optimizer, patience=4, factor=0.7, min_lr=1e-10, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.verbose = verbose\n",
    "        self.loss_history = []\n",
    "        self.prev_loss = [1000000] * 7\n",
    "        super(CustomLRScheduler, self).__init__(optimizer)\n",
    "\n",
    "    def step(self, avg_loss):\n",
    "        self.prev_loss.append(avg_loss)\n",
    "        if min(self.prev_loss[-(self.patience-1):]) > min(self.prev_loss[-self.patience:]):\n",
    "            self.prev_loss = [1000000] * 7\n",
    "            lr = 0.0\n",
    "            count = 0\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = max(param_group['lr'] * self.factor, self.min_lr)\n",
    "                lr+=param_group['lr']\n",
    "                count+=1\n",
    "            if self.verbose:\n",
    "                print(f'Reducing learning rate to:{lr/count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of learnable parameters:  3204000\n"
     ]
    }
   ],
   "source": [
    "from musicnet_model import MusicNetModel\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "scaler = GradScaler()\n",
    "model = MusicNetModel(window_size,n_hidden)\n",
    "#model.load_state_dict(torch.load('./model.pth'),strict=False)\n",
    "model.to(device)\n",
    "print(\"Count of learnable parameters: \",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "optimizer = torch_optimizer.RAdam(model.parameters(), lr=learning_rate,eps=epsilon,weight_decay=weight_decay) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=lr_plateau_factor,patience=lr_plateau_patience,verbose=True,mode='min',min_lr=5.0954e-05) #CustomLRScheduler(optimizer, factor=lr_plateau_factor,patience=lr_plateau_patience,verbose=True) \n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
    "loss_fn.to(device)\n",
    "\n",
    "step = 0\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with MusicNet(\"../../data\", train=True, download=False, window=window_size, epoch_size=EPOCH_SIZE, pitch_shift=0) as train_dataset,\\\n",
    "    MusicNet(\"../../data\", train=False, download=False, window=window_size, epoch_size=EVAL_SIZE, pitch_shift=0) as test_dataset:\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True,num_workers=12)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, drop_last=True,num_workers=4)\n",
    "    while epoch<=N_EPOCHS:\n",
    "        epoch+=1\n",
    "        losses = []\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(dtype=torch.float32):\n",
    "                loss, result = model(inputs.unsqueeze(-2).to(device),targets.to(device),loss_fn,SMOOTH)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0) works very bad \n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            step+=1\n",
    "            losses.append(loss.item())\n",
    "            if step%100==0:\n",
    "                avgLoss = np.mean(losses[-100:])\n",
    "                scheduler.step(avgLoss)\n",
    "            \n",
    "        print(f\"Train. Epoch {epoch}, loss: {np.mean(losses[-100:]):.3f}\")\n",
    "        if epoch%3==0:\n",
    "            all_targets = []\n",
    "            all_preds = []\n",
    "            model.eval()\n",
    "            for inputs, targets in test_loader:\n",
    "                with torch.no_grad():\n",
    "                    _,result = model(inputs.unsqueeze(-2).to(device),targets.to(device),loss_fn,SMOOTH)\n",
    "                    targets = targets[:, window_size//2, :].squeeze(1)\n",
    "                    all_targets += list(targets.numpy())\n",
    "                    all_preds += list(result.detach().cpu().numpy())\n",
    "\n",
    "            targets_np = np.array(all_targets)\n",
    "            preds_np = np.array(all_preds)\n",
    "            mask = targets_np.sum(axis=0) > 0\n",
    "            aps = average_precision_score(targets_np[:, mask], preds_np[:, mask])\n",
    "            print(f\"Epoch {epoch}. APS: {aps : .2%}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(BATCH_SIZE,1, window_size).cuda()\n",
    "torch.onnx.export(model, (dummy_input), 'rse_pytorch2048.onnx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
